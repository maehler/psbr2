---
title: "Statistical testing"
subtitle: "Practical Skills for Biology Research II"
author: "Niklas Mähler"
institute: "Umeå University"
date: "2021-10-19"
editor_options:
  chunk_output_type: console
---

layout: true

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
source(here::here("scripts/setup.R"))
setup_presentation("statistical_testing", filename = "20_statistical_testing.Rmd")
```

---

# Things we'll learn

- How a *p*-value is defined.
- Differences between parametric and non-parametric tests.
- How to choose an appropriate test depending on our question and our data.
- Basic understanding of linear models.

---

class: title

# *p*-values

---

# History

.pull-left[
- Date as far back as the 18th century.
  - [John Arbuthnot](https://en.wikipedia.org/wiki/John_Arbuthnot) tested the sex ratio of newborns.
- Formally introduced by [Karl Pearson](https://en.wikipedia.org/wiki/Karl_Pearson) with the $\chi^2$ test in the early 1900s.
- Popularised by [Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher) in his book [Statistical Methods for Research Workers](https://en.wikipedia.org/wiki/Statistical_Methods_for_Research_Workers) from 1925.
  - Also popularised the 0.05 significance level.
]

.pull-right[
> "From whence it follows, that it is Art, not Chance, that governs."
> .source[— John Arbuthnot]
]

???

---

# The lady tasting tea

.pull-left[
This was an experiment set up by Ronald Fisher in order to test the claim of [Muriel Bristol](https://en.wikipedia.org/wiki/Muriel_Bristol) that she could taste the difference in whether milk had been added to the tea, or the tea added to the milk.
She was given eight cups of tea with milk, where in four cups the milk had been added to the tea, and the four others the tea to the milk.

This test can be expressed as "what is the probability to correctly classify four cups of tea".
Mathematically, this can be expressed as

$$
\frac{1}{8 \choose 4} = \frac{1}{70} \approx 0.014
$$
The null hypothesis, i.e. that the woman would not be able to distinguish between the different ways of preparation, was rejected
]

.pull-right.center[
[![](https://upload.wikimedia.org/wikipedia/en/2/2d/The_Lady_Tasting_Tea_-_David_Salsburg.jpg)](https://en.wikipedia.org/wiki/The_Lady_Tasting_Tea)
]

???

The book is a nice popular science perspective on the evolution of modern-day statistics.

---

# The null hypothesis

The null hypothesis, often denoted $H_0$, is an important part of hypothesis testing.
We can see it as the counter-point to whatever we are trying to test.

Let's say that we are testing whether the mean height of trees is different whether or not they have been fertilised.
Before we perform the experiment, we should set up our hypotheses.
The null hypothesis, i.e. our negative outcome, would be that the mean height is the same:
$$
H_0: \mu_1 = \mu_2
$$
and the alternative hypothesis would be
$$
H_1: \mu_1 \neq \mu_2
$$

???

So, what is a null hypothesis?
In this particular example, we don't care which way the comparison goes, we just care that they are different.
In other words, this is a two-sided test.

If we have a particular question in mind, we need to try to formulate what a negative outcome of that test would be.

Many times in our field of study, we aren't doing classical hypothesis testing such as this, but rather take a more explorative approach.
Then it might not make sense to think of an hypothesis before we perform the experiment.

Still, we have to think about this whenever we do statistical testing.
Every test that we do will have a set null hypothesis, and we have to make sure that this applies to what we are testing.
If not, it is probably not an appropriate test for us, and if we still decide to use it, the results might not reflect our question.

---

# The definition

A *p*-value is defined as the probability of a result as extreme as that we observe, or more extreme, given the null hypothesis is true.

.pull-left[
Consider a value $t$ sampled from an unknown distribution $T$.
$$
p = \Pr(T \geq t~|~H_0)
$$

If the *p*-value is less than our significance threshold $\alpha$ (often 0.05), then we reject the null hypothesis.

`r note("Note that this is not the same as saying the null hypothesis is false. It is simply a way of rejecting a scenario that is unlikely given our data.", "warning")`
]

.pull-right[
```{r p_definition, echo = FALSE, out.width = "100%"}
tibble(
  x = seq(-4, 4, length.out = 200),
  y = dnorm(x)
) %>% 
  ggplot(aes(x, y)) +
  geom_area(fill = colorspace::lighten("forestgreen", amount = 0.2)) +
  geom_area(data = tibble(
    x = seq(2, 4, length.out = 50),
    y = dnorm(x)
  ), fill = "forestgreen") +
  annotate("segment", x = 2, y = 0, xend = 2, yend = dnorm(2),
           linetype = "dashed") +
  annotate("point", x = 2, y = 0, size = 2) +
  annotate("segment", x = 3, y = 0.1, xend = 2.5, yend = 0.03,
           arrow = arrow(type = "closed", length = unit(12, "pt"))) +
  annotate("text", x = 3, y = 0.11, label = "p-value",
           size = 10, vjust = 0) +
  labs(x = "Value", y = "Density") +
  theme_classic()
```
]

???

The overall shape of the distribution represents all possible observations, given that the null hypothesis is true.
In other words, if the null hypothesis is true, we expect most values to be close to zero.
The further out towards the edges we get, the more unlikely an observation becomes.

A small p-value can either mean that we are observing something very unlikely, or that our null hypothesis is false.
If the p-value is lower than our threshold level $\alpha$, then we usually reject the null hypothesis.
What this esentially means is that we deem this event so unlikely that we accept that our observation might come from a different distribution entirely.

---

# An example

.pull-left[
Here we have height measurements of 20 trees each from two populations, one which has been fertilised, and one that has not.

```{r tree_mean_heights, echo = FALSE}
tree_heights <- tibble(
  height = c(rnorm(20, mean = 250, sd = 50), rnorm(20, mean = 300, sd = 50)),
  population = gl(2, 20, labels = c("non-fertilised", "fertilised"))
) 

ggplot(tree_heights, aes(height, group = population, fill = population)) +
  geom_histogram(position = "identity", alpha = 0.5) +
  labs(x = "Tree height (cm)", y = "Number of trees")
```
]

.pull-right[
```{r, echo = FALSE}
tree_heights %>% 
  group_by(population) %>% 
  summarise(mean_height = mean(height),
            sd_height = sd(height)) %>% 
  knitr::kable()
```
]

???

We could do a two-sample t-test on this in order to compare the means.
Keep in mind that t-tests assume a normal distribution.
In this case I know that the values come from a normal distribution, but this we can't always know.
Therefore you should always check for normality, for example with a Q-Q plot.

---

# An example

.pull-left[
```{r, echo = FALSE}
tree_heights %>% 
  group_by(population) %>% 
  summarise(mean_height = mean(height),
            sd_height = sd(height)) %>% 
  knitr::kable()
```

$$
t = 
  \frac{\overline{X_1} - \overline{X_2}}{s_p \sqrt{\frac{2}{n}}} = 
  \frac{311.6 - 255.3}{\sqrt{\frac{57.6^2 + 64.8^2}{2}} \sqrt{\frac{2}{20}}}
  \approx 2.9
$$

This is a two-sided test, which means we also have to consider the other extreme.
$$
p = 2 \min{\left(\Pr(T \geq t~|~H_0), \Pr(T \leq t~|~H_0)\right)}
$$
The p-value is then the sum of the areas of the distribution from the observations to infinity.
$$
p \approx 0.006
$$
]

.pull-right[
The *t*-statistic for the difference in means follows a *t*-distribution.

```{r t_distribution, echo = FALSE, out.width = "100%", fig.height = 5}
tibble(
  x = seq(-5, 5, length.out = 200),
  y = dt(x, 38)
) %>% 
  ggplot(aes(x, y)) +
  geom_area(fill = colorspace::lighten("forestgreen", amount = 0.2)) +
  geom_area(data = tibble(x = seq(2.9, 5, length.out = 50), y = dt(x, 38)),
            fill = "forestgreen") +
  geom_area(data = tibble(x = seq(-2.9, -5, length.out = 50), y = dt(x, 38)),
            fill = "forestgreen") +
  annotate("segment", x = 2.9, y = 0, xend = 2.9, yend = dt(2.9, 38),
           linetype = "dashed") +
  annotate("point", x = 2.9, y = 0) +
  labs(x = "t-statistic",
       y = "Density") +
  theme_classic()
```
]

???

This test assumes that the samples come from a normal distribution, and that the variances are equal.

In this case, the distribution is symmetric around zero, so it doesn't matter which one we use.

This tail goes on to infinity, and this is also why we, at least theoretically, can't have p-values that are exactly zero or exactly one.

---

exclude: true

# The central limit theorem

---

exclude: true

# Why 0.05?

---

exclude: true

# What the *p*-value is not

- It is **not** the probability that the null hypothesis is true.

---

# Problems with multiple testing

.pull-left[
Another interpretation of the p-value is in terms of error rates.
We can say that at the $\alpha = 0.05$ level, there is a 5% chance that we reject the null hypothesis even though it is true.
The more tests we do, the more likely it is that we will see an extreme value purely by chance.

In these cases we have to adjust for *multiple testing*.
This means either modifying our p-values, or our significance level to reflect the number of tests that have been made.

If we do 100 tests at the $\alpha = 0.05$ level, then we expect to incorrectly reject the null hypothesis five times.
Compare this with performing a single test at the same confidence level and only having a 5% chance of rejecting the null hypothesis.
]

.pull-right[
`r knitr::include_url("https://imgs.xkcd.com/comics/significant.png", height = "500px")`
]

---

# Multiple testing correction in R

In R, we can correct for multiple testing with the `p.adjust` function.

.pull-left[
```{r}
pvals <- tibble(
  p = map_dbl(
    1:1000,
    ~ t.test(rnorm(20),
             rnorm(1000))$p.value
  ),
  padj = p.adjust(p)
)

sum(pvals$p < 0.05)
sum(pvals$padj < 0.05)
```
]

.pull-right.small[
```{r padjust_example}
pvals %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(value)) +
  facet_grid(cols = vars(name)) +
  geom_histogram() +
  geom_vline(xintercept = 0.05, colour = "firebrick2",
             linetype = "dashed") +
  labs(x = "p-value", y = "Number of observations")
```
]

???

`p.adjust` has a lot of different types of correction, but I will leave it to you to explore this.

---

class: title

# Types of tests

---

# Parametric tests

---

# Non-parametric tests

---

class: title

# How to choose a test

---

# What is the data we want to use for the test?

---

# What is it we want to test?

---

class: title

# Linear models

---

# What is a linear model?

---

# What about a linear model is linear?

---

# Suggested reading

- [R for Data Science: Modelling](https://r4ds.had.co.nz/model-intro.html)
- [An Introduction to Statistical Learning](https://www.statlearning.com/)
