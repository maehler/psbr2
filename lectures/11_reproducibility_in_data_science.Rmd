---
title: "Reproducibility in data science"
subtitle: "Practical Skills for Biology Research II"
author: "Niklas Mähler"
institute: "Umeå University"
date: "2021-10-07"
editor_options:
  chunk_output_type: console
---

```{r setup, include = FALSE}
source(here::here("scripts/setup.R"))
setup_presentation("reproducibility_in_data_science")
library(readxl)
```

class: title

# How do we define reproducibility?

???

There has been some confusion about what is being referred to when talking about reproducibility, so let's start with defining what we mean by reproducibility.

---

# Reproducibility vs repeatability vs replicability

These are all terms that you might run into, and there is some confusion surrounding them.
These definitions are taken from the [Association for Computing Machinery](https://www.acm.org/), and their document [Artifact Review and Badging](https://www.acm.org/publications/policies/artifact-review-and-badging-current).

.tile[
.tile-item.width33[
## Repeatability

.small[
> The measurement can be obtained with stated precision by the same team using the same measurement procedure, the same measuring system, under the same operating conditions, in the same location on multiple trials. For computational experiments, this means that a researcher can reliably repeat her own computation.
]
]

.tile-item.width33[
## Reproducibility

.small[
> The measurement can be obtained with stated precision by a different team using the same measurement procedure, the same measuring system, under the same operating conditions, in the same or a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using the author’s own artifacts.
]
]

.tile-item.width33[
## Replicability

.small[
> The measurement can be obtained with stated precision by a different team, a different measuring system, in a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using artifacts which they develop completely independently.
]
]
]

???

Here are some terms that are closely related, and sometimes used interchangeably in everyday English.

---

# Reproducibility vs repeatability vs replicability

.tile[
.tile-item.width33[
## Repeatability

- Same team
- Same experimental setup
]

.tile-item.width33[
## Reproducibility

- Different team
- Same experimental setup
]

.tile-item.width33[
## Replicability

- Different team
- Different experimental setup
]
]

[ACM: Artifact Review and Badging](https://www.acm.org/publications/policies/artifact-review-and-badging-current)

???

To simplify things a bit, this is essentially a summary of the previous slide.

The past 20 years or so, researchers have argued over what these definitions should be exactly.
As a matter of fact, if you look at the ACM document linked here, you will see that these definitions were updated as late as in August of 2020.

---

# Focus on what's relevant

Instead of splitting hairs over which of these should mean what, let's focus on *where* the reproducibility sits.

.tile[
.tile-item.width33[
## Methods reproducibility

> Provide sufficient detail about procedures and data so that the same procedures could be exactly repeated.
]

.tile-item.width33[
## Results reproducibility

> Obtain the same results from an independent study with procedures as closely matched to the original study as possible.
]

.tile-item.width33[
## Inferential reproducibility

> Draw the same conclusions from either an independent replication of a study or a reanalysis of the original study.
]
]

[Goodman *et al.* (2016): What does research reproducibility mean?](https://stm.sciencemag.org/content/8/341/341ps12)

[Plesser (2018): Reproducibility vs. Replicability: A Brief History of a Confused Terminology](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5778115/)

???

Instead of focusing on language, let's focus on the science instead, and what implications different types of reproducibility has on science.
This is exactly what Goodman *et al.* did in 2016 by definind three types of reproducibility: methods, results, and inferential reproducibility.

---

# What does reproducibility imply?

There are many different types of reproducibility

- In science: replicated study with similar inputs give the same result
  - *Treating patients with drug A reduces symptoms more than a placebo*
--
  
- In engineering: independent calculations reach the same conclusion
  - *Forces applied to this planned bridge are within the tolerated levels*
--

- In data science
  - Your code should run
  - Data should be preserved
  - Randomness should not be so random
  
???

So, even within data science we have different levels of reproducibility

---

class: title

# Why is reproducibility important?

???

I think you already have some ideas as to why reproducibility is important.

---

class: center

```{r reproducibility_crisis, echo = FALSE, fig.width = 12, fig.height = 10, out.width = "60%"}
reprod_data_url <- "https://ndownloader.figshare.com/files/5304313"
reprod_cols <- read_lines(reprod_data_url, n_max = 2) %>% 
  str_split("\t") %>% 
  setNames(c("names", "secondary")) %>% 
  as_tibble()
reprod_df <- read_tsv(reprod_data_url, skip = 2, col_names = FALSE)
crisis_df <- reprod_df %>% 
  count(X20) %>% 
  mutate(X20 = factor(X20, levels = c("I don't know", "There is no crisis of reproducibility",
                                      "There is a slight crisis of reproducibility",
                                      "There is a significant crisis of reproducibility"),
                      labels = c("I don't know",
                                 "No, there is no crisis",
                                 "Yes, a slight\ncrisis",
                                 "Yes, a\nsignificant crisis")),
         percent = 100 * n / sum(n)) %>% 
  arrange(desc(X20)) %>%
  mutate(min_percent = c(0, cumsum(percent[1:3])),
         max_percent = min_percent + percent,
         mid_percent = min_percent + (max_percent - min_percent) / 2)

ggplot(crisis_df) +
  ggforce::geom_arc_bar(aes(x0 = 0, y0 = 0, r0 = 0.7, r = 1,
                            amount = percent, fill = X20),
                        stat = "pie", colour = "transparent") +
  ggrepel::geom_text_repel(aes(x = sin(2*pi*mid_percent/100),
                               y = cos(2*pi*mid_percent/100),
                               label = str_c(round(percent), "%\n", X20)),
                           nudge_x = c(1, -1, -0.2, -0.3),
                           nudge_y = c(0, 0, 0.2, 0.5),
                           hjust = c(0, 1, 1, 0),
                           size = 8, lineheight = 0.93) +
  geom_text(data = tibble(n = sum(crisis_df$n)),
            mapping = aes(label = format(n, big.mark = ",")),
            x = 0, y = 0, size = 32, fontface = "bold",
            vjust = -0.25) +
  geom_text(data = tibble(n = sum(crisis_df$n)),
            x = 0, y = 0, size = 14, label = "researchers\nsurveyed",
            vjust = 1.1, lineheight = 0.9) +
  scale_fill_manual(values = c("#C8C8C8", "#5B8DBC", "#E57C88", "#D5363F")) +
  coord_fixed(xlim = c(-1.5, 1.8), ylim = c(-1, 1.3)) +
  ggtitle("IS THERE A REPRODUCIBILITY CRISIS?") +
  theme(
    complete = TRUE,
    legend.position = "none",
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.background = element_blank(),
    panel.grid = element_blank(),
    plot.title = element_text(face = "italic", size = 42, vjust = 3, colour = "gray40"),
    plot.margin = unit(c(-2, 0, -3, 0), units = "in")
  )
```

Reproduced from [1,500 scientists lift the lid on reproducibility](https://www.nature.com/articles/533452a)

???

This study asked over 1,500 scientists in various fields whether they think there is a reproducibility crisis.

Just to try to prove a point, I tried to reproduce all the figures in this presentation from the original data that was available on Figshare.
If you are interested, you can take a look in the Github repository for this course and see the code behind these slides.

---

```{r fail_to_reproduce, echo = FALSE, fig.height = 8, fig.width = 12}
reprod_df %>%
  select(91, 79, 80) %>%
  set_names(c("area", "My own", "Someone else's")) %>% 
  pivot_longer(2:3, names_to = "experiment") %>% 
  group_by(area, experiment) %>% 
  summarise(fraction = sum(value == "Yes") / n(), .groups = "drop") %>% 
  mutate(area = fct_reorder(as.factor(area), fraction, max)) %>% 
  ggplot(aes(x = area, y = fraction, group = experiment, fill = experiment)) + 
  geom_col(position = position_dodge(width = 0.87), width = 0.75) +
  scale_y_continuous(labels = scales::label_percent(), breaks = seq(0, 1, by = 0.2), expand = expansion()) +
  scale_fill_manual(values = c("#E48F90", "#DA373E")) +
  coord_flip(ylim = c(0, 1)) +
  labs(title = "HAVE YOU FAILED TO REPRODUCE\nAN EXPERIMENT?",
       subtitle = "Most scientists have experienced failure to reproduce results.") +
  theme(
    complete = TRUE,
    legend.position = "top",
    axis.title = element_blank(),
    legend.title = element_blank(),
    legend.justification = "left",
    legend.text = element_text(size = 16),
    panel.background = element_blank(),
    panel.grid = element_line(colour = "black", linetype = "dotted"),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    axis.line = element_line(colour = "black", size = 1, linetype = "solid"),
    axis.line.x = element_blank(),
    axis.text.x = element_text(size = 12, hjust = 0.3, vjust = -0.5),
    axis.text.y = element_text(size = 12, margin = margin(r = 3), vjust = 0.35, hjust = 1),
    plot.margin = unit(c(20, 20, 10, 15), units = "pt"),
    plot.title = element_text(size = 32, face = "italic", hjust = 0.5, margin = margin(b = 15)),
    plot.subtitle = element_text(size = 22, hjust = 0.5, margin = margin(b = 20))
  )
```

.center[
Reproduced from [1,500 scientists lift the lid on reproducibility](https://www.nature.com/articles/533452a)
]

???

A lot of it stems from the inability to successfully reproduce experiments.

---

```{r fail_to_reproduce_biology, echo = FALSE, fig.height = 8, fig.width = 12}
reprod_df %>%
  select(91, 79, 80) %>%
  set_names(c("area", "My own", "Someone else's")) %>% 
  pivot_longer(2:3, names_to = "experiment") %>% 
  group_by(area, experiment) %>% 
  summarise(fraction = sum(value == "Yes") / n(), .groups = "drop") %>% 
  mutate(area = fct_reorder(as.factor(area), fraction, max),
         fade = case_when(area == "Biology" ~ 1,
                           TRUE ~ 0.2)) %>% 
  ggplot(aes(x = area, y = fraction, group = experiment, fill = experiment, alpha = fade)) + 
  geom_col(position = position_dodge(width = 0.87), width = 0.75) +
  scale_y_continuous(labels = scales::label_percent(), breaks = seq(0, 1, by = 0.2), expand = expansion()) +
  scale_fill_manual(values = c("#E48F90", "#DA373E")) +
  scale_alpha_continuous(guide = guide_none(), range = c(0.2, 1)) +
  coord_flip(ylim = c(0, 1)) +
  labs(title = "HAVE YOU FAILED TO REPRODUCE\nAN EXPERIMENT?",
       subtitle = "Most scientists have experienced failure to reproduce results.") +
  theme(
    complete = TRUE,
    legend.position = "top",
    axis.title = element_blank(),
    legend.title = element_blank(),
    legend.justification = "left",
    legend.text = element_text(size = 16),
    panel.background = element_blank(),
    panel.grid = element_line(colour = "black", linetype = "dotted"),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    axis.line = element_line(colour = "black", size = 1, linetype = "solid"),
    axis.line.x = element_blank(),
    axis.text.x = element_text(size = 12, hjust = 0.3, vjust = -0.5),
    axis.text.y = element_text(size = 12, margin = margin(r = 3), vjust = 0.35, hjust = 1),
    plot.margin = unit(c(20, 20, 10, 15), units = "pt"),
    plot.title = element_text(size = 32, face = "italic", hjust = 0.5, margin = margin(b = 15)),
    plot.subtitle = element_text(size = 22, hjust = 0.5, margin = margin(b = 20))
  )
```

.center[
Reproduced from [1,500 scientists lift the lid on reproducibility](https://www.nature.com/articles/533452a)
]

???

I raise my hand for both of these categories.
I've failed to reproduce results of others, as well as my own results.

---

# Reproducibility in data science

.left-column[
## Your code should run
]

.right-column[
This might sound obvious, but it's not always so easy to make happen.

- Different software versions
- Different package versions
- Different operating systems
- For how long should your code run?
]

---

# Reproducibility in data science

.left-column[
## Your code should run
## Data should be preserved
]

.right-column[
Code can always be rewritten, and you can always devise new ways of analysing your data, but the data itself is a different story.
There are different options for storing of primary data.

- Public repositories for more or less specific data types
  - NCBI
  - ENA
  - SRA
  - Figshare
- Hard drives
- Cloud storage services
  - Dropbox
  - OneDrive
  - Google Drive
]

???

We'll go into more details when it comes to sharing data later in the course.

--

.right-column[
We'll talk more about how we can share data later in this course.
]

---

# Reproducibility in data science

.left-column[
## Your code should run
## Data should be preserved
## Randomness should not be so random
]

.right-column[
Oftentimes in data science, we use random numbers.
They could for example be part of estimating p-values for a particular analysis.
Due to the nature of (pseudo)random numbers, these will be different every time we generate them, but we can make sure that the same "random" numbers are generated every time by setting something called a seed.

```{r}
set.seed(12345)
rnorm(5)
rnorm(5)
set.seed(12345)
rnorm(5)
```
]

---

class: title

# Things that can help reproducibility

---

# RStudio projects

When working in R and RStudio, there is already something in place that will help you, and that's RStudio projects.

.pull-left[
- Create an RStudio project for each analysis
- Keep data there
- Keep scripts there
- Save results there (plots, reports, etc.)
- Only use relative paths
]

.pull-right[
![](https://d33wubrfki0l68.cloudfront.net/87562c4851bf4d0b0c8415dc9d6690493572e362/a6685/screenshots/rstudio-project-1.png)
]

???

When starting out with R, it's easy to view the environment, i.e. all the variables in your current session, as the result of all your hard work.
However, these are note worth very much if your session crashes, for example.
Then all those things will be lost.

Try to make a habit out of creating a new RStudio project for every analysis project that you're working on.
This course, for example, could be one project.

This would then mean that all the data belonging to a particular analysis is stored together with the source code that generates the results.
There might of course be cases where it is infeasible to share the project with the data, due to size or other constraints, but then we fall back to the FAIR principles.
As long as we make sure that our data is **F**indable, **A**ccessible, **Interoperable** and **R**eusable, then we can get away with just storing the metadata in the project.
This is likely much smaller than the actual data, and thus much easier to include in the project.

The scripts that we write will likely generate results in the form of cleaned datasets, plots, and reports.
We should make a habit out of generating these things **from scripts**, and not relying on copy-pasting, screenshots, or manually exporting data from RStudio.
Doing these things in code will make sure that you always have a way of regenerating your results.

This is usually what I do, for example when it comes to figures for papers.
I then try to have a single script associated with each figure of the paper.
Whenever, inevitably, your supervisor/PI tells you that you have to update this or that, then you can just go back to your script and change the relevant parameters, and have your new figure.

Absolute paths are not very useful from a reproducibility point of view.
It locks the analysis to a single system, since there is likely no one with the same directory structure as you.
If everything instead is stored in the project directory, and all paths are relative to the project directory, it can be shared as a single unit.

---

# Project structure

There are attempts at making standardised project structures for data science, like the [Cookiecutter Data Science project structure](https://drivendata.github.io/cookiecutter-data-science/).

.pull-left[
- `LICENSE`: tells others how they can use/reuse your data and/or code.
- `README.md`: a description of the project, including information on the structure, any prerequisites for running code, etc.
- `data`: where the data for the project is stored.
- `reports`: reports (and figures) generated by the scripts in the project.
- `scripts`: all scripts related to the project.
]

.pull-right[
```
`r emo::ji("folder")`project
 ├─── `r emo::ji("spiral notepad")`LICENSE
 ├─── `r emo::ji("spiral notepad")`README.md
 ├─── `r emo::ji("folder")`data
 ├─── `r emo::ji("folder")`docs
 ├─── `r emo::ji("folder")`reports
 |    ├── `r emo::ji("spiral notepad")` summary.html
 |    └── `r emo::ji("folder")`figures
 |        └── `r emo::ji("framed picture")` figure1.png
 └─── `r emo::ji("folder")` scripts
      └── `r emo::ji("spiral notepad")` summary.Rmd
```
]

???

This is just a possible setup.
As you go along, you will find something that works for you.
I think the important thing is to think about how you structure your projects, and not just store everything as a pile of files on your desktop.

---

# Session information

Sometimes it can be useful to share information about the R session that you have used in a particular analysis.

.small[
```{r}
sessionInfo()
```
]

???

The session information includes information on what version of R you're running, what operating system you're working on, and what packages are loaded.

---

# Session information

There is also a package called sessioninfo, with the function `session_info`:

.small[
```{r}
sessioninfo::session_info()
```
]

???

This is another way of listing session information.
It is similar to `sessionInfo`, but it excludes information that is rarely used, and instead adds other bits of information, like where packages were installed from.

---

# How to ask for help

.left-column[
## Different levels
]

.right-column[
```{r, error = TRUE}
x <- list(1, 2, 3, 4)
x[[5]]
```
]

???

Something that is related to reproducibility is when we are asking for help.
In order to get good help, we have to give our helper enough information in order for them to be able to **reproduce** whatever error you are experiencing.

Let's say you're running the code here, and you get this error.
There are many ways you can ask for help in this case.

--

.right-column[
- "It doesn't work."
]

???

This is superunhelpful.
Please never do this.

--

.right-column[
- "I get an error message saying 'subscript out of bounds'."
]

???

Ok, that's better.
They have realised that something is not working, and they have seen an error message, but maybe they don't know what that implies.

--

.right-column[
- "I have a list of integers that I'm trying to extract an element from, but I get an error message saying 'subscript out of bounds'."
]

???

Even better.

--

.right-column[
- "I have a list of 4 integers, and I'm trying to extract the fifth element, but I get an error message saying 'subscript out of bounds'."
]

???

Ok, now that's perfect, but on the other hand, they should have figured out what the error is by now.

Sometimes, the session information can contain important clues as to why something is not working as expected, so it can be good to include if nothing else helps.

---

# How to ask for help

.left-column[
## Different levels
## Rubber ducking
]

.right-column[
Explain your problem, out loud, to a rubber duck (or any other inanimate object).
Just the effort of trying to explain the issue to someone else might make you see a solution.
]

.right-column.center[
![:scale 60%, Enormous rubber duck floating in the water in a big city](https://live.staticflickr.com/3810/12671254994_37334d37cb_b.jpg)
]

---

# Writing methods sections

Oftentimes, the methods section in a published paper is the only thing we have in order to be able to reproduce methods, and more often than not, they are not very good.

Minimum to include:

- Any software used (including version numbers), properly cited.
- Any non-default parameters that were used.
- Non-base R packages used (including version numbers), properly cited

Ideally:

- All scripts that you used.

???

It's not uncommon that software is unpublished.
In these cases, just put a link to wherever the software was obtained.
If you wrote the software yourself, then definitely make it available to others.

---

# How to ask for help

.left-column[
## Different levels
## Rubber ducking
## reprex
]

.right-column[
The reprex package is for helping out making **repr**oducible **ex**amples.
It is a part of the tidyverse.

In essense, the package just helps you copy-paste code, but as opposed to simple copy-paste, it also runs it and generates any output as comments.
As a consequence, if not all dependencies of your code are fulfilled, it will fail.

It can work in a couple of different ways, but the way I have been using it is to copy the relevant code (<kbd>Ctrl</kbd>+<kbd>C</kbd> or <kbd>Cmd</kbd>+</kbd>C</kbd>) and running `reprex::reprex()`.
By default, this creates a markdown snippet that can be shared wherever you want to ask for help.

You can [read more about the reprex package on the tidyverse website](https://www.tidyverse.org/help/#reprex-pkg)
]

???

I will show a live demo of how this package works.

---

# Take-home messages

- Reproducibility is hard. Really hard.
- Not meant for you to fully grasp all the complexities related to reproducibility, but if I can get you to think about it now and then, that's a win to me.

---

# Suggested reading

- [R for Data Science: Workflow: projects](https://r4ds.had.co.nz/workflow-projects.html)
- [Cookiecutter Data Science project structure](https://drivendata.github.io/cookiecutter-data-science/)
- [State of Open Data](https://www.stateofopendata.od4d.net/)
  - [State of Open Data: 2020 report](https://digitalscience.figshare.com/articles/report/The_State_of_Open_Data_2020/13227875)